library(ggplot2)
library(tseries)
library(forecast)
# HAR Modelling & Forecasting
setwd('/Volumes/UNTITLED/MeÃÅmoire/Script')
# Importing Data (Daily RV)
Training_Data <- read_excel('daily_RVs.xlsx')[1 : 6279, c(1, 4)]
######################################################################################################
# Computing Lagged Daily RV
for (k in 2 : 6279){
Training_Data[k, 3] <- Training_Data[k - 1, 2]
k = k + 1
}
# Computing Weekly & Monthly RV's
for (i in 6 : 6279){
Training_Data[i, 4] <- (sum(Training_Data$RV[i - 5 : i])) / 5
i = i + 1
}
for (j in 22 : 6279){
Training_Data[j, 5] <- (sum(Training_Data$RV[j - 22 : j])) / 22
j = j + 1
}
colnames(Training_Data)[which(names(Training_Data) == "RV.1")] <- "Daily_Lag"
colnames(Training_Data)[which(names(Training_Data) == "V4")] <- "Weekly"
colnames(Training_Data)[which(names(Training_Data) == "V5")] <- "Monthly"
Testing_Data <- Training_Data[- c (1 : 6257), ]
Training_Data <- Training_Data[- c(1 : 22, 6257 : 6279), ]
#Data_xts <- xts(Data[, 2 : 4], order.by = as.Date(Data$date))
######################################################################################################
# Simple OLS
# OLS_HAR
OLS_HAR <- lm(RV ~ Daily_Lag + Weekly + Monthly, data = Training_Data)
summary(OLS_HAR)
# WLS_RV_HAR
w = c(1 / OLS_HAR$fitted.values)
WLS_RV_HAR <- lm(RV ~ Daily_Lag + Weekly + Monthly, weights = w, data = Training_Data)
summary(WLS_RV_HAR)
# WLS_GARCH_HAR
GARCH <- garch(OLS_HAR$residuals, order = c(1, 1))
w_GARCH <- c(1 / GARCH$fitted.values[, 1])
WLS_GARCH_HAR <- lm(RV ~ Daily_Lag + Weekly + Monthly, weights = w_GARCH, data = Training_Data)
summary(WLS_GARCH_HAR)
######################################################################################################
# Computing Residuals & error measures
OLS_HAR_test <- predict(OLS_HAR, newdata = Testing_Data)
WLS_RV_HAR_test <- predict(WLS_RV_HAR, newdata = Testing_Data)
WLS_GARCH_HAR_test <- predict(WLS_GARCH_HAR, newdata = Testing_Data)
OLS_HAR_Resid <- Testing_Data[, 2] - predict(OLS_HAR, newdata = Testing_Data)
WLS_RV_HAR_Resid <- Testing_Data[, 2] - predict(WLS_RV_HAR, newdata = Testing_Data)
WLS_GARCH_HAR_Resid <- Testing_Data[, 2] - predict(WLS_GARCH_HAR, newdata = Testing_Data)
Pred_Resid <- data.frame(OLS_HAR_Resid, WLS_RV_HAR_Resid, WLS_GARCH_HAR_Resid)
colnames(Pred_Resid) <- c('OLS_HAR_Resid', 'WLS_RV_HAR_Resid', 'WLS_GARCH_HAR_Resid')
OLS_HAR_MSE <- sum(Pred_Resid[, 1] ^ 2)
WLS_RV_MSE <- sum(Pred_Resid[, 2] ^ 2)
WLS_GARCH_MSE <- sum(Pred_Resid[, 3] ^ 2)
OLS_HAR_MAE <- sum(abs(Pred_Resid[, 1]))
WLS_RV_MAE <- sum(abs(Pred_Resid[, 2]))
WLS_GARCH_MAE <- sum(abs(Pred_Resid[, 3]))
######################################################################################################
# Forecast Plots
plot(OLS_HAR_test, type = 'l', ylim = c(0.000028, 0.00014), col = 'red')
lines(Testing_Data$RV, type ='l', col = 'blue', lty = 'dashed')
legend("topleft", c("Real Values", "Fitted"), fill = c("blue", "red"))
plot(WLS_RV_HAR_test, type = 'l', ylim = c(0.000028, 0.00013), col = 'red')
lines(Testing_Data$RV, type ='l', col = 'blue', lty = 'dashed')
legend("topleft", c("Real Values", "Fitted"), fill = c("blue", "red"))
plot(WLS_GARCH_HAR_test, type ='l', ylim = c(0.000028, 0.00013), col = 'red')
lines(Testing_Data$RV, type ='l', col = 'blue', lty = 'dashed')
legend("topleft", c("Real Values", "Fitted"), fill = c("blue", "red"))
######################################################################################################
# DM Test
dm.test(OLS_HAR_test, WLS_RV_HAR_test, alternative = 'greater')
dm.test(OLS_HAR_test, WLS_GARCH_HAR_test, alternative = 'greater')
dm.test(WLS_RV_HAR_test, WLS_GARCH_HAR_test, alternative = 'greater')
# Cumulative Squarred Forecast Error
for (i in 1 : 22){
Pred_Resid[i, 4] <- cumsum(Pred_Resid[1 : i, 1]^2)
}
plot(Pred_Resid[, 4])
View(Pred_Resid)
Pred_Resid[i, 4] <- cumsum(Pred_Resid[1 : 1 + i, 1]^2)
View(Pred_Resid)
print(i)
i = i + 1
for (i in 1 : 22){
Pred_Resid[i, 4] <- cumsum(Pred_Resid[1 : 1 + i, 1]^2)
i = i + 1
print(i)
}
View(Pred_Resid)
for (i in 1 : 23){
Pred_Resid[i, 4] <- cumsum(Pred_Resid[1 : 1 + i, 1]^2)
i = i + 1
print(i)
}
View(Pred_Resid)
for (i in 1 : 21){
Pred_Resid[i, 4] <- cumsum(Pred_Resid[1 : 1 + i, 1]^2)
i = i + 1
print(i)
}
View(Pred_Resid)
plot(Pred_Resid[, 4])
for (i in 1 : 21){
Pred_Resid[i, 4] <- Pred_Resid[i, 1]^2
i = i + 1
}
plot(Pred_Resid[, 4])
Pred_Resid[i, 4] <- cumsum(Pred_Resid[i, 4])
View(Pred_Resid)
Pred_Resid[i, 4] <- cumsum(Pred_Resid[1 : i, 4])
Pred_Resid[i, 4] <- cumsum(Pred_Resid[1 : 1 + i, 4])
View(Pred_Resid)
Pred_Resid[i, 4] <- Pred_Resid[i - 1, 4] + Pred_Resid[i, 4]
View(Pred_Resid)
for (i in 1 : 21){
Pred_Resid[i, 4] <- Pred_Resid[i, 1]^2
Pred_Resid[i, 4] <- Pred_Resid[i - 1, 4] + Pred_Resid[i, 4]
i = i + 1
}
for (i in 1 : 21){
Pred_Resid[i, 4] <- Pred_Resid[i, 1]^2
Pred_Resid[i, 4] <- Pred_Resid[1 : i, 4] + Pred_Resid[i, 4]
i = i + 1
}
for (i in 1 : 21){
Pred_Resid[i, 4] <- Pred_Resid[i, 1]^2
Pred_Resid[i, 4] <- Pred_Resid[1 : 1 + i, 4] + Pred_Resid[i, 4]
i = i + 1
}
View(Pred_Resid)
plot(Pred_Resid[, 4])
Pred_Resid[, 4] <- cumsum(Pred_Resid[, 1])
Pred_Resid[, 4] <- cumsum((Pred_Resid[, 1]^2))
View(Pred_Resid)
plot(Pred_Resid[, 4])
Pred_Resid[, 4] <- cumsum((Pred_Resid[, 1]^2) - (Pred_Resid[, 2]^2))
plot(Pred_Resid[, 4])
Pred_Resid[, 4] <- cumsum((Pred_Resid[, 1]^2) - (Pred_Resid[, 2]^2))
Pred_Resid[, 5] <- cumsum((Pred_Resid[, 1]^2) - (Pred_Resid[, 3]^2))
Pred_Resid[, 6] <- cumsum((Pred_Resid[, 2]^2) - (Pred_Resid[, 3]^2))
plot(Pred_Resid[, 4], type = 'l', col = 'darkcyan')
plot(Pred_Resid[, 5], type = 'l', col = 'darkcyan')
plot(Pred_Resid[, 6], type = 'l', col = 'darkcyan')
Pred_Resid[, 4] <- cumsum((Pred_Resid[, 1]^2) - (Pred_Resid[, 2]^2))
Pred_Resid[, 5] <- cumsum((Pred_Resid[, 1]^2) - (Pred_Resid[, 3]^2))
Pred_Resid[, 6] <- cumsum((Pred_Resid[, 2]^2) - (Pred_Resid[, 3]^2))
Pred_Resid[, 7] <- cumsum((Pred_Resid[, 3]^2) - (Pred_Resid[, 1]^2))
plot(Pred_Resid[, 4], type = 'l', col = 'darkcyan')
plot(Pred_Resid[, 5], type = 'l', col = 'darkcyan')
plot(Pred_Resid[, 6], type = 'l', col = 'darkcyan')
plot(Pred_Resid[, 7], type = 'l', col = 'darkcyan')
plot(Pred_Resid[, 6], type = 'l', col = 'darkcyan')
plot(Pred_Resid[, 5], type = 'l', col = 'darkcyan')
plot(Pred_Resid[, 4], type = 'l', col = 'darkcyan')
library(shiny)
Data <- read.csv("Data.csv")
setwd("/Users/lucasA/Desktop/ML-Class-/Random Forests")
Data <- read.csv("Data.csv")
library()
library()setwd("/Users/lucasA/Desktop/ML-Class-/Random Forests")
View(Data)
install.packages("rpart")
install.packages("randomForest")
install.packages("gbm")
install.packages("adabag")
library(rpart)
library(randomForest)
library(gbm)
library(adabag)
library(caret)
Data <- read.csv("Data.csv", header = TRUE, sep = ';')
head(data)
headt(Data)
head(Data)
View(Data)
ggplot(data = Data, aes(x = id, y = SEXEQ))+
geom_line ()+
theme_minimal()
ggplot(data = Data, aes(x = id, y = SEXEQ))+
geom_histogram()+
theme_minimal()
ggplot(data = Data, aes(x = SEXEQ))+
geom_histogram()+
theme_minimal()
ggplot(data = Data, aes(x = id, y = SEXEQ))+
geom_histogram()+
theme_minimal()
View(Data)
ggplot(data = Data, aes(x = id, y = RELAT))+
geom_histogram()+
theme_minimal()
ggplot(data = Data, aes(x = RELAT))+
geom_histogram()+
theme_minimal()
ggplot(data = Data, aes(x = RELAT))+
geom_histogram(bins = 50, color = 'navyblue')+
theme_minimal()
ggplot(data = Data, aes(x = RELAT))+
geom_histogram(bins = 50, color = 'navyblue', fill = 'navyblue')+
theme_minimal()
ggplot(data = Data, aes(x = RELAT))+
geom_histogram(bins = 50, color = 'black', fill = 'navyblue')+
theme_minimal()
View(Data)
ggplot(data = Data, aes(x = AGER))+
geom_histogram(bins = 50, color = 'black', fill = 'navyblue')+
theme_minimal()
library(summarytools)
install.packages("summarytools")
library(summarytools)
view(dfsummary(Data))
dfsummary(Data)
dfSummary(Data)
library(summarytools)
install.packages("summary")
library(summarytools)
view(dfSummary(Data))
install.packages("tidyverse")
view(Data)
par(mfrow = c(3, 3))
for (i in 3 : 11){
boxplot(Data[, i])
}
boxplot(Data[, i] ~ Data$CARVP, main = colnames)
View(Data)
for (i in 3 : 11){
boxplot(Data[, i] ~ Data$CARVP, main = colnames(Data)[i])
}
View(Data)
for (i in 4 : 11){
boxplot(Data[, i] ~ Data$CARVP, main = colnames(Data)[i])
}
for (i in 4 : 12){
boxplot(Data[, i] ~ Data$CARVP, main = colnames(Data)[i])
}
par(mfrow = c(3, 3))
for (i in 4 : 12){
boxplot(Data[, i] ~ Data$CARVP, main = colnames(Data)[i])
}
rpart_control <- (minbucket = 5, cp = 0.01, xval = 5)
rpart_control <- rpart.control(minbucket = 5, cp = 0.01, xval = 5)
library(rpart)
rpart_control <- rpart.control(minbucket = 5, cp = 0.01, xval = 5)
rpart_fit <- rpart(CARVP ~ ., method = 'class', control = rpart_control, data = Data)
summmary(rpart_fit)
summary(rpart_fit)
par(xpd = NA)
plot(rpart_fit, uniform = FALSE)
text(rpart_fit, all = TRUE, une.n = TRUE, cex = 0.8)
text(rpart_fit, all = TRUE, use.n = TRUE, cex = 0.8)
par(xpd = NA)
plot(rpart_fit, uniform = FALSE)
text(rpart_fit, all = TRUE, use.n = TRUE, cex = 0.8)
par(xpd = NA)
plot(rpart_fit, uniform = FALSE)
text(rpart_fit, all = TRUE, use.n = TRUE, cex = 0.8)
par(xpd = NA)
plot(rpart_fit, uniform = FALSE)
text(rpart_fit, all = TRUE, use.n = TRUE, cex = 0.8)
par(mfrow = c(3, 3))
par(xpd = NA)
plot(rpart_fit, uniform = FALSE)
text(rpart_fit, all = TRUE, use.n = TRUE, cex = 0.8)
plot(rpart_fit, uniform = FALSE)
text(rpart_fit, all = TRUE, use.n = TRUE, cex = 0.8)
par(xpd = NA)
plot(rpart_fit, uniform = FALSE)
text(rpart_fit, all = TRUE, use.n = TRUE, cex = 0.8)
par(xpd = NA)
plot(rpart_fit, uniform = FALSE)
text(rpart_fit, all = TRUE, use.n = TRUE, cex = 0.8)
library(partykit)
install.packages("partykit")
library(partykit)
partykit::party-plot(as.party(rpart_fit))
detach('package::partykit', unload = TRUE)
plotcp(rpart_fit)
printcp(rpart_fit)
cp <- rpart_fit$cptable[3, 1]
rpart_fit_prune <- prune(rpart_fit, cp = cp)
partykit::party-plot(as.party(rpart_fit_prune))
detach('package::partykit', unload = TRUE)
predict.rpart(rpart_fit_prune)
predict(rpart_fit_prune)
pred <- predict(rpart_fit_prune)
table(Data$CARVP, pred)
length(pred)
pred <- predict(rpart_fit_prune, type = 'class')
table(Data$CARVP, pred)
1 - sum(diag(table) / sum(table))
pred <- predict(rpart_fit_prune, type = 'class')
tab <- table(Data$CARVP, pred)
1 - sum(diag(tab) / sum(tab))
rpart_control <- rpart.control(minbucket = 5, cp = 0.01, xval = 5)
rpart_fit <- rpart(CARVP ~ ., method = 'class', control = rpart_control, data = boot)
cp <- rpart_fit$cptable[3, 1]
rpart_fit_prune <- prune(rpart_fit, cp = cp)
partykit::party-plot(as.party(rpart_fit))
detach('package::partykit', unload = TRUE)
rpart_control_boot <- rpart.control(minbucket = 5, cp = 0.01, xval = 5)
rpart_fit_boot <- rpart(CARVP ~ ., method = 'class', control = rpart_control, data = boot)
cp <- rpart_fit_boot$cptable[3, 1]
rpart_fit_prune_boot <- prune(rpart_fit_boot, cp = cp)
partykit::party-plot(as.party(rpart_fit_prune_boot))
detach('package::partykit', unload = TRUE)
rpart_control_boot <- rpart.control(minbucket = 5, cp = 0.01, xval = 5)
rpart_fit_boot <- rpart(CARVP ~ ., method = 'class', control = rpart_control_boot, data = boot)
cp <- rpart_fit_boot$cptable[3, 1]
rpart_fit_prune_boot <- prune(rpart_fit_boot, cp = cp)
partykit::party-plot(as.party(rpart_fit_prune_boot))
detach('package::partykit', unload = TRUE)
boot <- sample(1 : n, replace = TRUE)
OOB <-  stediff(1 : n, unique(boot))
rpart_control_boot <- rpart.control(minbucket = 5, cp = 0.01, xval = 5)
rpart_fit_boot <- rpart(CARVP ~ ., method = 'class', control = rpart_control_boot, data = boot)
cp <- rpart_fit_boot$cptable[3, 1]
rpart_fit_prune_boot <- prune(rpart_fit_boot, cp = cp)
partykit::party-plot(as.party(rpart_fit_prune_boot))
detach('package::partykit', unload = TRUE)
boot <- sample(1 : n, replace = TRUE)
boot <- sample(1:n, replace = TRUE)
boot <- sample(1 : n, replace = TRUE)
library(rpart)
library(randomForest)
library(gbm)
library(adabag)
library(caret)
library(partykit)
setwd("/Users/lucasA/Desktop/ML-Class-/Random Forests")
Data <- read.csv("Data.csv", header = TRUE, sep = ';')
ggplot(data = Data, aes(x = RELAT))+
geom_histogram(bins = 50, color = 'black', fill = 'navyblue')+
theme_minimal()
ggplot(data = Data, aes(x = AGER))+
geom_histogram(bins = 50, color = 'black', fill = 'navyblue')+
theme_minimal()
par(mfrow = c(3, 3))
for (i in 4 : 12){
boxplot(Data[, i] ~ Data$CARVP, main = colnames(Data)[i])
}
# CART
rpart_control <- rpart.control(minbucket = 5, cp = 0.01, xval = 5)
rpart_fit <- rpart(CARVP ~ ., method = 'class', control = rpart_control, data = Data)
summary(rpart_fit)
par(xpd = NA)
plot(rpart_fit, uniform = FALSE)
text(rpart_fit, all = TRUE, use.n = TRUE, cex = 0.8)
partykit::party-plot(as.party(rpart_fit))
detach('package::partykit', unload = TRUE)
plotcp(rpart_fit)
printcp(rpart_fit)
cp <- rpart_fit$cptable[3, 1]
rpart_fit_prune <- prune(rpart_fit, cp = cp)
partykit::party-plot(as.party(rpart_fit_prune))
detach('package::partykit', unload = TRUE)
pred <- predict(rpart_fit_prune, type = 'class')
tab <- table(Data$CARVP, pred)
1 - sum(diag(tab) / sum(tab)) # taux d'erreur
boot <- sample(1 : n, replace = TRUE)
OOB <-  stediff(1 : n, unique(boot))
rpart_control_boot <- rpart.control(minbucket = 5, cp = 0.01, xval = 5)
rpart_fit_boot <- rpart(CARVP ~ ., method = 'class', control = rpart_control_boot, data = boot)
cp <- rpart_fit_boot$cptable[3, 1]
rpart_fit_prune_boot <- prune(rpart_fit_boot, cp = cp)
partykit::party-plot(as.party(rpart_fit_prune_boot))
detach('package::partykit', unload = TRUE)
boot <- sample(1 : n, replace = TRUE)
n <- nrow(Data)
pX <- ncol(Data) - 1
valntree <- 2000
valmtry <- floor(sqrt(pX))
valnodedsize <- 1
rdf <- randomForest(CRAVP ~ ., data = Data, ntree = valntree,
mtry = valmtry, nodesize = valnodedsize, important = TRUE,
proximity = TRUE, nPerm = 1)
rdf <- randomForest(CARVP ~ ., data = Data, ntree = valntree,
mtry = valmtry, nodesize = valnodedsize, important = TRUE,
proximity = TRUE, nPerm = 1)
print(rdf)
plot(rdf)
library(rpart)
library(randomForest)
library(gbm)
library(adabag)
library(caret)
library(partykit)
setwd("/Users/lucasA/Desktop/ML-Class-/Random Forests")
Data <- read.csv("Data.csv", header = TRUE, sep = ';')
n <- nrow(Data)
ggplot(data = Data, aes(x = RELAT))+
geom_histogram(bins = 50, color = 'black', fill = 'navyblue')+
theme_minimal()
ggplot(data = Data, aes(x = AGER))+
geom_histogram(bins = 50, color = 'black', fill = 'navyblue')+
theme_minimal()
par(mfrow = c(3, 3))
for (i in 4 : 12){
boxplot(Data[, i] ~ Data$CARVP, main = colnames(Data)[i])
}
# CART
rpart_control <- rpart.control(minbucket = 5, cp = 0.01, xval = 5)
rpart_fit <- rpart(CARVP ~ ., method = 'class', control = rpart_control, data = Data)
summary(rpart_fit)
par(xpd = NA)
plot(rpart_fit, uniform = FALSE)
text(rpart_fit, all = TRUE, use.n = TRUE, cex = 0.8)
partykit::party-plot(as.party(rpart_fit))
detach('package::partykit', unload = TRUE)
plotcp(rpart_fit)
printcp(rpart_fit)
cp <- rpart_fit$cptable[3, 1]
rpart_fit_prune <- prune(rpart_fit, cp = cp)
partykit::party-plot(as.party(rpart_fit_prune))
detach('package::partykit', unload = TRUE)
pred <- predict(rpart_fit_prune, type = 'class')
tab <- table(Data$CARVP, pred)
1 - sum(diag(tab) / sum(tab)) # taux d'erreur
boot <- sample(1 : n, replace = TRUE)
OOB <-  stediff(1 : n, unique(boot))
rpart_control_boot <- rpart.control(minbucket = 5, cp = 0.01, xval = 5)
rpart_fit_boot <- rpart(CARVP ~ ., method = 'class', control = rpart_control_boot, data = boot)
cp <- rpart_fit_boot$cptable[3, 1]
rpart_fit_prune_boot <- prune(rpart_fit_boot, cp = cp)
partykit::party-plot(as.party(rpart_fit_prune_boot))
detach('package::partykit', unload = TRUE)
# Random Forest
pX <- ncol(Data) - 1
valntree <- 2000
valmtry <- floor(sqrt(pX))
valnodedsize <- 1
rdf <- randomForest(CARVP ~ ., data = Data, ntree = valntree,
mtry = valmtry, nodesize = valnodedsize, important = TRUE,
proximity = TRUE, nPerm = 1)
print(rdf)
plot(rdf)
on.exit(par)
plot(rdf)
print(rdf)
on.exit(par)
plot(rdf)
str(Data$CARVP)
tuneRF(x = Data[2 : 13], y = Data$CARVP, mtryStart = 3,
ntreeTry = 500, stepFactor = 2, improve = 0.001)
tuneRF(x = Data[, 2 : 13], y = Data$CARVP, mtryStart = 3,
ntreeTry = 500, stepFactor = 2, improve = 0.001)
fit.control <- trainControl(method = 'repeatedcv', number = 5, repeats = 10,
classProbs = TRUE, summaryFunction = twoClassSummary,
search = 'grid')
deb <- Sys.time()
rdf_grid <- train(CARVP ~ ., data = Data, method = 'rf', metric = 'Accuracy',
tuneGrid = tune.mtry, trControl = fit.control)
fin <- Sys.time()
print(fin - deb)
fit.control <- trainControl(method = 'repeatedcv', number = 5, repeats = 10,
classProbs = TRUE, summaryFunction = twoClassSummary,
search = 'grid')
tune.mtry <- expand.grid(.mtry = (1 : 10))
deb <- Sys.time()
rdf_grid <- train(CARVP ~ ., data = Data, method = 'rf', metric = 'Accuracy',
tuneGrid = tune.mtry, trControl = fit.control)
fin <- Sys.time()
print(fin - deb)
print(rdf.grid)
print(rdf_grid)
plot(rdf_grid)
confusionMatrix(rdf_grid)
fit.control <- trainControl(method = 'repeatedcv', number = 5, repeats = 10,
classProbs = TRUE, summaryFunction = twoClassSummary,
search = 'grid')
tune.mtry <- expand.grid(.mtry = (1 : 10))
deb <- Sys.time()
rdf_grid <- train(CARVP ~ ., data = Data, method = 'rf', metric = 'Accuracy',
tuneGrid = tune.mtry, trControl = fit.control)
fin <- Sys.time()
print(fin - deb)
print(rdf_grid)
plot(rdf_grid)
pX <- ncol(Data) - 1
valntree <- 500
valmtry <- 2
valnodedsize <- 1
rdf <- randomForest(CARVP ~ ., data = Data, ntree = valntree,
mtry = valmtry, nodesize = valnodedsize, important = TRUE,
proximity = TRUE, nPerm = 1)
print(rdf)
plot(rdf)
predict(rdf)
print(rdf)
pred <- predict(rdf, newdata = Data)
pred_OOB <- predict(rdf)
pred <- predict(rdf, newdata = Data)
pred_OOB <- predict(rdf)
tab <- table(Data$CARVP, pred)
tab_OOB <- table(Data$CARVP, pred_OOB)
1 - sum(diag(tab) / sum(tab))
1 - sum(diag(tab_OOB) / sum(tab_OOB))
confusionMatrix(predict(rdf), Data$CARVP, positive = levels(Data$CARVP)[2], mode = 'sens_spec')
varImpPlot(rdf)
importance(rdf)
importance(rdf, scale = TRUE)
fit_gbm <-gbm(Data$CARVP ~ ., data = Data, distribution = 'multinomial', interaction.depth = 2,
n.tress = 1000, shrinkage = 0.01, bag.fraction = 0.8, cv.folds = 5)
fit_gbm <-gbm(Data$CARVP ~ ., data = Data, distribution = 'multinomial', interaction.depth = 2,
n.trees = 1000, shrinkage = 0.01, bag.fraction = 0.8, cv.folds = 5)
fit_gbm
str(Data$SEXEQ)
fir_gbm$cv.error[best.iter]
fit_gbm$cv.error[best.iter]
View(fit_gbm)
best.iter = gbm.perf(fit_gbm, method = 'cv')
fit_gbm$cv.error[best.iter]
